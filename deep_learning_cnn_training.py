# -*- coding: utf-8 -*-
"""deep_learning_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1blgHcYt4GEWta8IkGxmSO0UElyT_p-Ay
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GlobalMaxPooling2D
import pickle
from tqdm import tqdm
import gc

# Function to preprocess images using tf.data
def preprocess_image(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [224, 224])
    img = preprocess_input(img)
    return img

# Function to make a dataset
def make_dataset(image_paths, batch_size):
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)
    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    return dataset

# Initialize model
base_model = ResNet50(weights='imagenet', include_top=False)
model = Sequential([base_model, GlobalMaxPooling2D()])
model.trainable = False

# File paths
image_directory_path = '/kaggle/Images_data/images'
filenames = [os.path.join(image_directory_path, f) for f in os.listdir(image_directory_path) if f.endswith(('.png', '.jpg', '.jpeg'))]

# Create dataset
batch_size = 16
dataset = make_dataset(filenames, batch_size)

# Feature extraction
all_features = []
for images_batch in tqdm(dataset, desc="Extracting features"):
    batch_features = model.predict(images_batch)
    batch_features = batch_features.reshape((batch_features.shape[0], -1))
    normalized_features = batch_features / np.linalg.norm(batch_features, axis=1, keepdims=True)
    all_features.extend(normalized_features)

    # Memory cleanup
    del images_batch, batch_features, normalized_features
    gc.collect()

# Save the extracted features and filenames for later use
with open('/content/drive/My Drive/embeddings.pkl', 'wb') as f:
    pickle.dump(all_features, f)
with open('/content/drive/My Drive/filenames.pkl', 'wb') as f:
    pickle.dump(filenames, f)

print("Feature extraction completed.")

!unzip -q /content/drive/MyDrive/Images_data.zip -d /kaggle/Images_data



"""# Save the extracted features and filenames for later use
with open('/content/drive/My Drive/embeddings.pkl', 'wb') as f:
    pickle.dump(all_features, f)
with open('/content/drive/My Drive/filenames.pkl', 'wb') as f:
    pickle.dump(filenames, f)
"""

